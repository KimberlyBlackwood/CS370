# #TODO: Complete the Q-Training Algorithm Code Block

    This is your deep Q-learning implementation. The goal of your deep Q-learning implementation is to find the best possible navigation sequence that results in reaching the treasure cell while maximizing the reward. In your implementation, you need to determine the optimal number of epochs to achieve a 100% win rate.

   Pseudocode:
    
    For each epoch:
        Reset the environment at a random starting cell
        agent_cell = randomly select a free cell
        
        Hint: Review the reset method in the TreasureMaze.py class.
    
        Set the initial environment state
        env_state should reference the environment's current state
        Hint: Review the observe method in the TreasureMaze.py class.

        While game status is not game over:
           previous_envstate = env_state
            Decide on an action:
                - If possible, take a random valid exploration action and 
                  randomly choose action (left, right, up, down)
                  and assign it to an action variable
                - Else, pick the best exploitation action from the model and assign it to an action variable
                  Hint: Review the predict method in the GameExperience.py class.
    
           Retrieve the values below from the act() method.
           env_state, reward, game_status = qmaze.act(action)
           Hint: Review the act method in the TreasureMaze.py class.
    
            Track the wins and losses from the game_status using win_history 
         
           Store the episode below in the Experience replay object
           episode = [previous_envstate, action, reward, envstate, game_status]
           Hint: Review the remember method in the GameExperience.py class.
        
           Train neural network model and evaluate loss
           Hint: Call GameExperience.get_data to retrieve training data (input and target) 
           and pass to the train_step method and assign it to batch_loss and append to the loss variable
        
      If the win rate is above the threshold and your model passes the completion check, that would be your epoch.

Note: A 100% win rate DOES NOT EXPLICITLY MEAN that you have solved the maze. It simply indicates that during the last evaluation, the pirate happened to get to the treasure. Be sure to utilize the completion_check()function to validate your pirate found the treasure at every starting point and consistently! 

You will need to complete the section starting with #START_HERE. Please use the pseudocode above as guidance.  


ABOVE IS THE PSEUDOCODE FOR DEEP Q-LEARNING

def qtrain(model, maze, **opt):
    """Deep Q-Learning for maze solving with early stopping and validation."""
    global epsilon 
    n_epoch = opt.get('n_epoch', 5000)
    max_memory = opt.get('max_memory', 16*maze.size)
    data_size = opt.get('data_size', 64)
    
    start_time = datetime.datetime.now()
    qmaze = TreasureMaze(maze)
    target_model = clone_model(model)
    target_model.set_weights(model.get_weights())
    experience = GameExperience(model, target_model, max_memory=max_memory)
    
    win_history = []
    hsize = qmaze.maze.size // 2
    loss = 0.0
    n_episodes = 0
    
    # Epsilon-greedy parameters
    epsilon = 1.0
    epsilon_decay = 0.995
    epsilon_min = 0.01
    
    solved_epoch = None  # Track when BOTH conditions met
    
    for epoch in range(n_epoch):
        # Force (0,0) training 10% of time for completion_check compatibility
        if epoch % 10 == 0 and (0, 0) in qmaze.free_cells:
            agent_cell = (0, 0)
        else:
            agent_cell = random.choice(qmaze.free_cells)
            
        qmaze.reset(agent_cell)
        env_state = qmaze.observe()
        
        episode_won = False
        steps = 0
        max_steps = 100
        
        while steps < max_steps:
            previous_env_state = env_state
            valid_actions = qmaze.valid_actions()
            
            if not valid_actions:
                break
            
            # (1) EPSILON-GREEDY: Balance exploration vs exploitation
            if np.random.rand() < epsilon:
                action = random.choice(valid_actions)  # Explore
            else:
                state = np.asarray(previous_env_state, dtype=np.float32)
                if state.ndim == 1:
                    state = np.expand_dims(state, axis=0)
                q_values = model(state, training=False).numpy()
                action = np.argmax(q_values[0])
                if action not in valid_actions:
                    action = random.choice(valid_actions)  # Exploit
            
            # Take action and get reward
            env_state, reward, game_status = qmaze.act(action)
            done = (game_status in ['win', 'lose'])
            
            # (2) EXPERIENCE REPLAY: Store transition for later training
            episode = [previous_env_state, action, reward, env_state, done]
            experience.remember(episode)
            n_episodes += 1
            
            # (3) TRAIN: Sample minibatch and update Q-network
            if len(experience.memory) > data_size:
                inputs, targets = experience.get_data()
                if inputs is not None and len(inputs) > 0:
                    batch_loss = train_step(inputs, targets)
                    loss += float(batch_loss)
            
            steps += 1
            
            if game_status == 'win':
                episode_won = True
                break
            elif game_status == 'lose':
                break
        
        win_history.append(1 if episode_won else 0)
        win_rate = sum(win_history[-hsize:]) / hsize if len(win_history) >= hsize else 0.0
        
        # Update epsilon
        epsilon = max(epsilon * epsilon_decay, epsilon_min)
        
        # (4) STOP CONDITION: BOTH win_rate ≥ 0.999 AND completion_check True
        if win_rate >= 0.999 and epoch % 50 == 0:  # Test every 50 epochs
            print(f"\n Epoch {epoch}: Testing completion_check...")
            is_solved = completion_check(model, maze)
            print(f"   Win rate: {win_rate:.4f} | completion_check: {is_solved}")
            
            if is_solved:
                solved_epoch = epoch
                print(f"SOLVED at epoch {epoch}!")
                print(f"completion_check(model, maze) = True")
                print(f"Final win rate: {win_rate:.4f}")
                break
        
        # Update target network
        if epoch % 25 == 0:
            target_model.set_weights(model.get_weights())
        
        # Logging
        if epoch % 50 == 0:
            avg_loss = loss / max(1, n_episodes)
            t = format_time((datetime.datetime.now() - start_time).total_seconds())
            print(f"Epoch {epoch:03d} | Win: {win_rate:.3f} | Loss: {avg_loss:.4f} | ε: {epsilon:.3f} | Time: {t}")
    
    # FINAL VALIDATION CONFIRMATION
    if solved_epoch is not None:
        print(f"\n TRAINING SUCCESSFULLY COMPLETED!")
        print(f"Optimal training length: {solved_epoch} epochs")
        final_check = completion_check(model, maze)
        print(f"Final validation: completion_check(model, maze) = {final_check}")
    else:
        print("\n Training completed without meeting both criteria")
        print(f"Final win rate: {win_rate:.3f}")
    
    print("Training complete!")

def format_time(seconds):
    """Convert training time to human-readable format."""
    if seconds < 400:
        s = float(seconds)
        return "%.1f seconds" % (s,)
    elif seconds < 4000:
        m = seconds / 60.0
        return "%.2f minutes" % (m,)
    else:
        h = seconds / 3600.0
        return "%.2f hours" % (h,)

                               
